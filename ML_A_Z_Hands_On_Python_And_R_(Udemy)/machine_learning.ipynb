{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Data-Processing\" data-toc-modified-id=\"Data-Processing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Processing</a></div><div class=\"lev2 toc-item\"><a href=\"#Read-in-dataset\" data-toc-modified-id=\"Read-in-dataset-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Read in dataset</a></div><div class=\"lev2 toc-item\"><a href=\"#Take-care-of-missing-data-using-Imputer\" data-toc-modified-id=\"Take-care-of-missing-data-using-Imputer-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Take care of missing data using Imputer</a></div><div class=\"lev2 toc-item\"><a href=\"#Encode-categorical-data-using-LabelEncoder-and-OneHotEncoder\" data-toc-modified-id=\"Encode-categorical-data-using-LabelEncoder-and-OneHotEncoder-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Encode categorical data using LabelEncoder and OneHotEncoder</a></div><div class=\"lev2 toc-item\"><a href=\"#Split-data-into-training/test-set-using-train_test_split\" data-toc-modified-id=\"Split-data-into-training/test-set-using-train_test_split-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Split data into training/test set using train_test_split</a></div><div class=\"lev2 toc-item\"><a href=\"#Feature-Scaling-using-StandardScalar\" data-toc-modified-id=\"Feature-Scaling-using-StandardScalar-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Feature Scaling using StandardScalar</a></div><div class=\"lev2 toc-item\"><a href=\"#Data-Processing-Template\" data-toc-modified-id=\"Data-Processing-Template-16\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Data Processing Template</a></div><div class=\"lev1 toc-item\"><a href=\"#Regression\" data-toc-modified-id=\"Regression-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Regression</a></div><div class=\"lev2 toc-item\"><a href=\"#Simple-Linear-Regression\" data-toc-modified-id=\"Simple-Linear-Regression-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Simple Linear Regression</a></div><div class=\"lev2 toc-item\"><a href=\"#Multiple-Linear-Regression\" data-toc-modified-id=\"Multiple-Linear-Regression-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Multiple Linear Regression</a></div><div class=\"lev2 toc-item\"><a href=\"#Polynomial-Regression\" data-toc-modified-id=\"Polynomial-Regression-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Polynomial Regression</a></div><div class=\"lev2 toc-item\"><a href=\"#Support-Vector-Regression\" data-toc-modified-id=\"Support-Vector-Regression-24\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Support Vector Regression</a></div><div class=\"lev2 toc-item\"><a href=\"#Decision-Tree-Regression\" data-toc-modified-id=\"Decision-Tree-Regression-25\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Decision Tree Regression</a></div><div class=\"lev2 toc-item\"><a href=\"#Random-Forest-Regression\" data-toc-modified-id=\"Random-Forest-Regression-26\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Random Forest Regression</a></div><div class=\"lev2 toc-item\"><a href=\"#Evaluation-of-Regression-Models\" data-toc-modified-id=\"Evaluation-of-Regression-Models-27\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Evaluation of Regression Models</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile README.md\n",
    "# Machine Learning Course by Udemy 2017 Summer\n",
    "\n",
    "\n",
    "## Chapter 1: Data Processing\n",
    "1.1 Read in dataset  \n",
    "1.2 Take care of missing data using imputer  \n",
    "1.3 Encode categorical data using LabelEncoder and OneHotEncoder  \n",
    "1.4 Split data into training/test set using train_test_split  \n",
    "1.5 Feature Scaling using StandardScalar  \n",
    "1.6 Data Processing Template  \n",
    "\n",
    "## Chapter 2: Regression  \n",
    "2.1 Simple Linear Regression  \n",
    "2.2 Multiple Linear Regression  \n",
    "2.3 Polynomial Regression  \n",
    "2.4 Support Vector Regression  \n",
    "2.5 Decision Tree Regression  \n",
    "2.6 Random Forest Regression  \n",
    "2.7 Evaluation of Regression Models  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: May 2, 2017\n",
    "# Data Processing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataset\n",
    "dataset = pd.read_csv('data/Data.csv')\n",
    "X = dataset.iloc[:,:-1].values\n",
    "X2 = dataset.iloc[:,:-1].values\n",
    "y = dataset.iloc[:,3].values\n",
    "DF(X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take care of missing data using Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take care of missing data\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Long method\n",
    "#imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "#imputer = imputer.fit(X[:, 1:3])\n",
    "#X[:,1:3] = imputer.transform(X[:,1:3])\n",
    "\n",
    "# one liner\n",
    "X[:, 1:3] = Imputer(missing_values='NaN', strategy='mean', axis=0\n",
    "                   ).fit_transform(X[:,1:3])\n",
    "DF(X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode categorical data using LabelEncoder and OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical values\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "\n",
    "le_X = LabelEncoder()\n",
    "X[:,0] = le_X.fit_transform(X[:,0])\n",
    "\n",
    "# Create dummy encoder since France=0 is not smaller than Spain=1 or so on\n",
    "ohc = OneHotEncoder()\n",
    "#?OneHotEncoder\n",
    "X = ohc.fit_transform(X).toarray()\n",
    "le_y = LabelEncoder()\n",
    "y = le_y.fit_transform(y)\n",
    "DF(X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training/test set using train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#?train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling using StandardScalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "# This gives deprecation warning for StandardScalar\n",
    "# To solve this make the argument as argument.reshape(-1,1),astype(float)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# sc_X = StandardScaler()\n",
    "# X_train = sc_X.fit_transform(X_train)\n",
    "# X_test = sc_X.transform(X_test)\n",
    "# sc_y = StandardScaler()\n",
    "# y_train = sc_y.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train.reshape(-1,1).astype(float))\n",
    "X_test = sc.transform(X_test.reshape(-1,1).astype(float))\n",
    "y_train = sc.fit_transform(y_train.reshape(-1,1).astype(float))\n",
    "DF(y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proc_template='''\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 3].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "\"\"\"from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "sc_y = StandardScaler()\n",
    "y_train = sc_y.fit_transform(y_train)\"\"\"\n",
    "\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proc_template='''\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Data.csv')\n",
    "y = dataset.iloc[:, 3].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "\"\"\"from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train.reshape(-1,1).astype(float))\n",
    "X_test = sc.transform(X_test.reshape(-1,1).astype(float))\n",
    "y_train = sc.fit_transform(y_train.reshape(-1,1).astype(float))\"\"\"\n",
    "\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile src/regression/simple_linear_regression.py\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Author      : Bhishan Poudel; Physics PhD Student, Ohio University\n",
    "# Date        : May 09, 2017\n",
    "# Last update : July 1, 2017\n",
    "#\n",
    "#\n",
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame as DF\n",
    "\n",
    "# Import dataset\n",
    "dataset = pd.read_csv('data/Salary_Data.csv')\n",
    "# print(DF(dataset))\n",
    "\n",
    "X = dataset.iloc[:,:-1].values\n",
    "y = dataset.iloc[:,1].values\n",
    "\n",
    "# print(DF(X))\n",
    "\n",
    "\n",
    "# Split data into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ?train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 1/3, random_state = 0)\n",
    "\n",
    "# Fitting the data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Visualize\n",
    "plt.scatter(X_train, y_train, c='r')\n",
    "plt.plot(X_train, regressor.predict(X_train), c='b')\n",
    "plt.title('Salary vs Experience (Training Set)')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary')\n",
    "# plt.show()\n",
    "\n",
    "# Visualize Test set\n",
    "# plt.scatter(X_test,y_test,c='r')\n",
    "# plt.plot(X_test, regressor.predict(X_test), c='b')\n",
    "# plt.title('Salary vs Experience (Test Set)')\n",
    "# plt.xlabel('Years of Experience')\n",
    "# plt.ylabel('Salary')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile src/regression/multiple_linear_regression.py\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import DataFrame as DF\n",
    "\n",
    "# Import dataset\n",
    "dataset = pd.read_csv('data/50_Startups.csv')\n",
    "# print(dataset.head())\n",
    "# dataset.describe()\n",
    "\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 4].values\n",
    "\n",
    "\n",
    "# Encoding\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Encode New York to 2, California to 0 and Florida is 1.\n",
    "le = LabelEncoder()\n",
    "X[:, 3] = le.fit_transform(X[:, 3])\n",
    "# print(DF(X).head())\n",
    "\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder(categorical_features = [3])\n",
    "X = ohe.fit_transform(X).toarray()\n",
    "# print(DF(X).head())\n",
    "\n",
    "\n",
    "# Avoid Dummy Variable Trap\n",
    "# We drop the California column\n",
    "# Here catagorical feature of one variable becomes three\n",
    "# Third value depend on first two values, e.g. 00 then must be 1.\n",
    "# Dummy variable always increase the degree of freedom.\n",
    "# http://www.statsblogs.com/2013/11/09/multicollinearity-and-collinearity-in-multiple-regression-a-tutorial/\n",
    "X = X[:, 1:]\n",
    "# print(DF(X).head())\n",
    "\n",
    "# Split data into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "\n",
    "# Fitting\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Build the optimal model using Backward Elimination\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "\n",
    "# Add a column of 50 ones at the beginning of the X\n",
    "X = np.append(arr=np.ones((50,1)).astype(int),  values=X, axis=1  )\n",
    "# print(DF(X).head())\n",
    "\n",
    "\n",
    "# Initialize optimal X as original matrix, we will backdrop later\n",
    "# OLS is ordinary least sq\n",
    "X_opt = X[:,[0,1,2,3,4,5]]  \n",
    "regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit() \n",
    "# print(regressor_OLS.summary())\n",
    "\n",
    "# Look for highest Probability (lowest significance) and remove this\n",
    "# remove if it is above 5%.\n",
    "X_opt = X[:,[0,1,3,4, 5]]\n",
    "regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit() \n",
    "# print(regressor_OLS.summary())\n",
    "\n",
    "\n",
    "# remove if it is above 5%.\n",
    "X_opt = X[:,[0,3,4,5]]\n",
    "regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit() \n",
    "# print(regressor_OLS.summary())\n",
    "\n",
    "\n",
    "# remove if it is above 5%.\n",
    "X_opt = X[:,[0,3]]\n",
    "regressor_OLS = sm.OLS(endog=y, exog=X_opt).fit() \n",
    "# print(regressor_OLS.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "189px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
