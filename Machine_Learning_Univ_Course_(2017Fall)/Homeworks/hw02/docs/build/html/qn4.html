<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Polynomial Regression &#8212; Bhishan&#39;s 1 documentation</title>
    
    <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/sidebar.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Stochastic Gradient Descent" href="qn5.html" />
    <link rel="prev" title="Multivariate Linear Regression" href="qn3.html" /> 
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="qn5.html" title="Stochastic Gradient Descent"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="qn3.html" title="Multivariate Linear Regression"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Bhishan&#39;s 1 documentation</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="polynomial-regression">
<h1>Polynomial Regression<a class="headerlink" href="#polynomial-regression" title="Permalink to this headline">Â¶</a></h1>
<p>In this question we fit the polynomial regression with and
without L2 regularization (Ridge Regression) using gradient descent method.</p>
<p>a) The input data is noisy sinusoidal curve generated using
the formula</p>
<div class="math">
<p><img src="_images/math/36890339cd9f6755390f8510152fd73ec68efd30.png" alt="t(x) = sin(2 \pi x) + x (x + 1) / 4 + \epsilon"/></p>
</div><p>Where epsilon varies from 0 to 0.005 with Gaussian mean zero and variance 0.005.</p>
<p>The output textfile dataset.txt has 30 examples. It is randomly divided into
three equal parts into the files train.txt, test.txt and devel.txt.</p>
<p>b) I saved and plotted three data sets separately inside the folder images.
hw02qn4_train.png, hw02qn4_test.png, and hw02qn4_devel.png.</p>
<p>For the simplicity I also created a single plot showing all these subplots in a
single plot (images/hw02qn4b.png).</p>
<img alt="_images/hw02qn4b.png" src="_images/hw02qn4b.png" />
<ol class="loweralpha simple" start="3">
<li>The ridge regularized objective function is given below.</li>
</ol>
<div class="math">
<p><img src="_images/math/b535c4de5e7548c056448cb3eeb2d6d51882c4bd.png" alt="J(w) = \frac{1}{2N} \sum_{n=1}^N (h(x_n,w) - t_n)^2 + \
\frac{\lambda}{2} ||w||^2"/></p>
</div><p>The algorithm to minimize the cost function J(w) using gradient descent method is:</p>
<div class="math">
<p><img src="_images/math/3596efd576a1f6e1ba45643911a6a72b03334874.png" alt="w^{\tau + 1} = w^{\tau} - \eta \nabla J"/></p>
</div><p>Here tau is iteration number and eta is the learning rate.
The gradient of cost function for regularized case is given below.</p>
<div class="math">
<p><img src="_images/math/6897b50c4ccd8cb5081f26db0f23e21cf99d7057.png" alt="\nabla J = \frac{1}{N} \sum_{n=1}^N (h - t) X + \lambda w"/></p>
</div><p>From class lectures I included the formulas to calulate BGD and SGD.</p>
<img alt="_images/BGD.png" src="_images/BGD.png" />
<img alt="_images/SGD.png" src="_images/SGD.png" />
<p>d) In this problem of polynomial curve fitting I chose the degree of the polynomial
to be 5 (that gave the lowest RMSE in previous homework).</p>
<p>d1i) I used the six different learning rates to chose the best learning rate
hyperparameter from training data. The plot of J(w) versus epochs is given below.</p>
<p>From the figure I chose 0.1 to be the smooth and high enough learning rate for
this problem.</p>
<img alt="_images/cost_epochs_good_lr.png" src="_images/cost_epochs_good_lr.png" />
<img alt="_images/cost_epochs_bad_lr.png" src="_images/cost_epochs_bad_lr.png" />
<p>d1 ii) I selected the learning rate to be 0.1 and ran the gradient descent as
long as J(w) decreases by at least 0.001 after each epoch.</p>
<p>For different stepsizes I got different final epoch to get the converged value.
The plot is given below:</p>
<img alt="_images/cost_history_bgd_unreg.png" src="_images/cost_history_bgd_unreg.png" />
<p>d1 iii) Here we compare the parameter vector w from normal equation, gradient
descent, and stochastic gradient descent methods:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="c1"># NOTE: I used threshold = 0.001 not 1e-10 for Unregularized cases (bgd and sgd)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="c1"># for SGD to get same results</span>
<span class="n">shrinkage</span> <span class="o">=</span> <span class="mf">0.00</span> <span class="n">final_iter</span> <span class="o">=</span> <span class="mi">238</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.10</span> <span class="n">deg</span> <span class="o">=</span> <span class="mi">5</span> <span class="n">threshold</span> <span class="o">=</span> <span class="mf">1.00e-04</span>
<span class="n">w_norm_eqn</span>  <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1711</span>  <span class="mf">1.6086</span>  <span class="mf">5.8353</span>  <span class="o">-</span><span class="mf">35.1236</span>  <span class="mf">43.5586</span>  <span class="o">-</span><span class="mf">16.1171</span> <span class="p">]]</span>
<span class="n">w_unreg_bgd</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1711</span>  <span class="o">-</span><span class="mf">0.0314</span>  <span class="o">-</span><span class="mf">0.6621</span>  <span class="o">-</span><span class="mf">0.3889</span>  <span class="mf">0.1045</span>  <span class="mf">0.5930</span> <span class="p">]]</span>
<span class="n">w_unreg_sgd</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1747</span>  <span class="o">-</span><span class="mf">0.0231</span>  <span class="o">-</span><span class="mf">0.6722</span>  <span class="o">-</span><span class="mf">0.3940</span>  <span class="mf">0.1107</span>  <span class="mf">0.6114</span> <span class="p">]]</span>
</pre></div>
</div>
<p>d2) With regularization.
With threshold of 1e-10 I calculated the cost history for regularized case.
The regularization parameter (shrinkage) value was chosen to be 0.1.</p>
<img alt="_images/cost_history_bgd_reg.png" src="_images/cost_history_bgd_reg.png" />
<p>d2 iii) For regulrized case comparison of normal, bgd and sgd is given below::
For regularized case with threshold of 1e-10, at final iteration 657 the cost
function values converges.:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="c1"># for SGD to get same results</span>
<span class="n">shrinkage</span>  <span class="o">=</span> <span class="mf">0.10</span> <span class="n">final_iter</span> <span class="o">=</span> <span class="mi">657</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.10</span> <span class="n">deg</span> <span class="o">=</span> <span class="mi">5</span> <span class="n">threshold</span> <span class="o">=</span> <span class="mf">1.00e-10</span>
<span class="n">w_norm_eqn</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1711</span>  <span class="o">-</span><span class="mf">0.1535</span>  <span class="o">-</span><span class="mf">0.3607</span>  <span class="o">-</span><span class="mf">0.1988</span>  <span class="mf">0.0442</span>  <span class="mf">0.2763</span> <span class="p">]]</span>
<span class="n">w_reg_bgd</span>  <span class="o">=</span> <span class="p">[[</span><span class="mf">0.1555</span>  <span class="o">-</span><span class="mf">0.1537</span>  <span class="o">-</span><span class="mf">0.3606</span>  <span class="o">-</span><span class="mf">0.1987</span>  <span class="mf">0.0442</span>  <span class="mf">0.2762</span> <span class="p">]]</span>
<span class="n">w_reg_sgd</span>  <span class="o">=</span> <span class="p">[[</span><span class="mf">0.0890</span>  <span class="o">-</span><span class="mf">0.1079</span>  <span class="o">-</span><span class="mf">0.1054</span>  <span class="o">-</span><span class="mf">0.0680</span>  <span class="o">-</span><span class="mf">0.0268</span>  <span class="mf">0.0098</span> <span class="p">]]</span>
</pre></div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h4>Previous topic</h4>
  <p class="topless"><a href="qn3.html"
                        title="previous chapter">Multivariate Linear Regression</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="qn5.html"
                        title="next chapter">Stochastic Gradient Descent</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/qn4.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="qn5.html" title="Stochastic Gradient Descent"
             >next</a> |</li>
        <li class="right" >
          <a href="qn3.html" title="Multivariate Linear Regression"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Bhishan&#39;s 1 documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2017, Bhishan Poudel.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.5.6.
    </div>
  </body>
</html>